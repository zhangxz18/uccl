# UCCL RDMA NCCL/RCCL

UCCL RDMA plugin for NCCL/RCCL.

1. UCCL supports two network fabrics: RoCE, Infiniband.
2. UCCL supports two modes: Unreliable Connection (UC) and Reliable Connection (RC).
3. UCCL supports both Nvidia and AMD GPUs.

## Configuration
### Env Vars
By default, UCCL auto-detects all necessary parameters, similar to NCCL/RCCL. 
But you can overwrite these parameters, eg:
```
UCCL_IB_HCA:          The names of IB devices you want to use.
UCCL_SOCKET_IFNAME:   The control NIC name you want to use for bootstrapping.
```

For convenience, the NCCL versions, i.e., NCCL_IB_HCA and NCCL_SOCKET_IFNAME, are also available.

### run_nccl_test.sh:
```
HOSTNAME:               The MPI host file (e.g., hosts_single_process, hosts_multi_process)

HCA_NAMES:              For UCCL_IB_HCA.
CTRL_NIC:               For UCCL_SOCKET_IFNAME.

Usage: ./run_nccl_test.sh [NCCL/UCCL: 0/1, default:1] [# of total processes, default:2] [# of GPUs per process, default:8] [allreduce/alltoall: 0/1] [# of processes per node, default:1] [MPI host file]
```

### run_rccl_test.sh: 
```
NODEFILE:               The MPI host file (auto-generated by run_rccl_test.sh)

HCA_NAMES:              For UCCL_IB_HCA.
CTRL_NIC:               For UCCL_SOCKET_IFNAME.

Usage: ./run_rccl_test.sh [RCCL/UCCLL: rccl/uccl, default:uccl]
```

## Building and running UCCL for Nvidia GPUs

Build `nccl` and `nccl-tests`: 

```bash
# Eg, /home/yangz/uccl
export UCCL_HOME=<the path of uccl>

# Build nccl (taking ~3min); assume H100 GPUs
cd $UCCL_HOME/thirdparty/nccl
make src.build -j NVCC_GENCODE="-gencode=arch=compute_90,code=sm_90"
cp src/include/nccl_common.h build/include/

# Build nccl-tests; consider "conda deactivate" when hitting dependency errors
cd $UCCL_HOME/thirdparty/nccl-tests
make MPI=1 MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi CUDA_HOME=/usr/local/cuda NCCL_HOME=$UCCL_HOME/thirdparty/nccl/build -j
```

Build `libnccl-net-uccl.so`

```bash
cd $UCCL_HOME/rdma
make -j
```

Running `nccl-tests`:

```bash
cd $UCCL_HOME/scripts
python rsync.py

cd $UCCL_HOME/rdma
# Edit hosts_single_process/hosts_multi_process to fill up the node addresses. 

# 1) 8 GPUs per process
./run_nccl_test.sh 1 2 8 1
# 2) 1 GPU per process
./run_nccl_test.sh 1 16 1 1 8 hosts_multi_process
```


## Building and running UCCL for AMD GPUs

This guide assumes under the [AMD HPC Fund cluster](https://amdresearch.github.io/hpcfund/hardware.html), without any root access. 

Prepare dependency: install and activate recent Anaconda to prepare necessary libraries such as `-lglog -lgflags -lgtest`. Consider installing it into `$WORK` directory as Anaconda is large. 

Then inside the conda env, install libs that contains glog, gflags, and gtest: 
```bash
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.3
```

Build `rccl` and `rccl-tests`: 

```bash
# Export the path of uccl
export UCCL_HOME="/home1/yangzhou/uccl"
# Export the path of anaconda lib
export CONDA_LIB_HOME="/work1/yzhou/yangzhou/anaconda3/lib"

# Avoiding gfx950 as the HPC Fund cluster clang does not support it yet. Note this takes ~20min. 
cd $UCCL_HOME/thirdparty/rccl
./install.sh --amdgpu_targets="gfx906;gfx908;gfx90a;gfx942;gfx1030;gfx1100;gfx1101;gfx1102;gfx1200;gfx1201" -j 16

cd $UCCL_HOME/thirdparty/rccl-tests
make MPI=1 MPI_HOME=/opt/ohpc/pub/mpi/openmpi4-gnu12/4.1.5 HIP_HOME=/opt/rocm-6.3.1 NCCL_HOME=/opt/rocm-6.3.1/include/rccl CUSTOM_RCCL_LIB=/opt/rocm-6.3.1/lib/librccl.so -j
```

Build `librccl-net-uccl.so`

```bash
cd $UCCL_HOME/rdma
make -f MakefileHip -j
```

Run `rccl-tests`:

```bash
# Using slurm to allocate two AMD nodes
salloc -N 2 -n 2 -p mi2104x -t 00:30:00

# Usage: ./run_rccl_test_hpcfund.sh [rccl/uccl, default: uccl]
./run_rccl_test_hpcfund.sh rccl
```

### If your are using Broadcom NICs

Using the following to build `librccl-net-uccl.so`

```bash
cd $UCCL_HOME/rdma
make -f MakefileHip -j
```

Run `rccl-tests`:

```bash
# Edit scripts/node_ips/amd.txt to fill up the node addresses. 

# Usage: ./run_rccl_test.sh [rccl/uccl, default: uccl]
./run_rccl_test.sh rccl
```

## Environment Variables in UCCL

UCCL supports the following environment variables to configure.

For example, one can enlarge the chunk size to 128KB by setting `UCCL_CHUNK_SIZE_KB=128`. 

Use `UCCL_PARAM()` to introduce new environment variables.

| Environment Variable | Description | Default Value |
|---------------------|-------------|---------------|
| UCCL_IB_HCA | The names of IB devices used | (null) |
| UCCL_SOCKET_IFNAME | The control NIC name used for bootstrapping | (null) |
| UCCL_PIN_TO_NUMA | Pin threads to the NUMA node | 1 |
| UCCL_ROCE_TRAFFIC_CLASS | Traffic class for RoCE | 3 |
| UCCL_ROCE_SERVICE_LEVEL | Service level for RoCE | 135 |
| UCCL_IB_SERVICE_LEVEL | Service level for IB | 0 |
| UCCL_RCMODE | Use RC for data transfer | 1 (Broadcom NIC), 0 (others) |
| UCCL_BYPASS_PACING | Bypass the pacing stage | true |
| UCCL_NUM_ENGINES | Number of engines per device | 4 (NVIDIA), 1 (AMD) |
| UCCL_PORT_ENTROPY | Path/QP per engine | 32 (NVIDIA), 256 (AMD) |
| UCCL_CHUNK_SIZE_KB | Maximum chunk size for each WQE | 64 (NVIDIA), 128 (AMD) |
